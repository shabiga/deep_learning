{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Object Detection\n",
    "\n",
    "In this notebook, we will build a simple object detection model with two heads:\n",
    "\n",
    "- a classification head to predict the class of the object in the image\n",
    "- a localization head to predict the bounding box of the object in the image\n",
    "\n",
    "We will use the Pascal VOC 2007 dataset, which contains 20 classes of objects. We will only use 5 classes: \"dog\", \"cat\", \"bus\", \"car\", \"aeroplane\". To get started, we will use a pre-trained ResNet50 model to precompute the convolutional representations of the images. We will then build a simple model to predict the class and the bounding box of the object in the image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install \"imageio[pyav]\""
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we start, it's important that if you're on Google Colab, you've enabled the GPU. To do this, go to `Runtime` > `Change runtime type` and select `GPU` from the `Hardware accelerator` dropdown.\n",
    "\n",
    "The following code cell will check if you have a GPU available. If you don't, you will still be able to run the notebook, but the code will take much longer to execute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.gpu_device_name() == '':\n",
    "    print('You do not have a GPU available.')\n",
    "else:\n",
    "    print('You have a GPU available.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "URL_VOC = (\"http://host.robots.ox.ac.uk/pascal/VOC/\"\n",
    "           \"voc2007/VOCtrainval_06-Nov-2007.tar\")\n",
    "FILE_VOC = \"VOCtrainval_06-Nov-2007.tar\"\n",
    "FOLDER_VOC = \"VOCdevkit\"\n",
    "\n",
    "if not op.exists(FILE_VOC):\n",
    "    print('Downloading from %s to %s...' % (URL_VOC, FILE_VOC))\n",
    "    urlretrieve(URL_VOC, './' + FILE_VOC)\n",
    "\n",
    "if not op.exists(FOLDER_VOC):\n",
    "    print('Extracting %s...' % FILE_VOC)\n",
    "    tar = tarfile.open(FILE_VOC)\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "\n",
    "URL_REPRESENTATIONS = (\"https://github.com/m2dsupsdlclass/lectures-labs/\"\n",
    "                       \"releases/download/0.2/voc_representations.h5\")\n",
    "FILE_REPRESENTATIONS = \"voc_representations.h5\"\n",
    "\n",
    "if not op.exists(FILE_REPRESENTATIONS):\n",
    "    print('Downloading from %s to %s...'\n",
    "          % (URL_REPRESENTATIONS, FILE_REPRESENTATIONS))\n",
    "    urlretrieve(URL_REPRESENTATIONS, './' + FILE_REPRESENTATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Localization model\n",
    "\n",
    "The objective is to build and train a classification and localization network. This exercise will showcase the flexibility of Deep Learning with several, heterogenous outputs (bounding boxes and classes)\n",
    "\n",
    "We will build the model in three consecutive steps:\n",
    "- **Extract label annotations** from a standard Object Detection dataset, namely **Pascal VOC 2007**;\n",
    "- Use a pre-trained image classification model (namely ResNet50) to **precompute convolutional representations** with shape `(7, 7, 2048)` for all the images in the object detection training set;\n",
    "- **Design and train a baseline object detection model with two heads** to predict: \n",
    "  - class labels (5 possible classes)\n",
    "  - bounding box coordinates of a single detected object in the image\n",
    "\n",
    "## Loading images and annotations\n",
    "\n",
    "We will be using Pascal VOC 2007, a dataset widely used in detection and segmentation http://host.robots.ox.ac.uk/pascal/VOC/ To lower memory footprint and training time, we'll only use 5 classes: \"dog\", \"cat\", \"bus\", \"car\", \"aeroplane\". Here are the first steps:\n",
    "- Load the annotations file from pascalVOC and parse it (xml file)\n",
    "- Keep only the annotations we're interested in, and containing a single object\n",
    "- Pre-compute ResNet conv5c from the corresponding images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xml.etree.ElementTree as etree\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "# Parse the xml annotation file and retrieve the path to each image,\n",
    "# its size and annotations\n",
    "def extract_xml_annotation(filename):\n",
    "    z = etree.parse(filename)\n",
    "    objects = z.findall(\"./object\")\n",
    "    size = (int(z.find(\".//width\").text), int(z.find(\".//height\").text))\n",
    "    fname = z.find(\"./filename\").text\n",
    "    dicts = [{obj.find(\"name\").text:[int(obj.find(\"bndbox/xmin\").text), \n",
    "                                     int(obj.find(\"bndbox/ymin\").text), \n",
    "                                     int(obj.find(\"bndbox/xmax\").text), \n",
    "                                     int(obj.find(\"bndbox/ymax\").text)]} \n",
    "             for obj in objects]\n",
    "    return {\"size\": size, \"filename\": fname, \"objects\": dicts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters annotations keeping only those we are interested in\n",
    "# We only keep images in which there is a single item\n",
    "annotations = []\n",
    "\n",
    "filters = [\"dog\", \"cat\", \"bus\", \"car\", \"aeroplane\"]\n",
    "idx2labels = {k: v for k, v in enumerate(filters)}\n",
    "labels2idx = {v: k for k, v in idx2labels.items()}\n",
    "\n",
    "annotation_folder = \"VOCdevkit/VOC2007/Annotations/\"\n",
    "for filename in sorted(os.listdir(annotation_folder)):\n",
    "    annotation = extract_xml_annotation(op.join(annotation_folder, filename))\n",
    "\n",
    "    new_objects = []\n",
    "    for obj in annotation[\"objects\"]:\n",
    "        # keep only labels we're interested in\n",
    "        if list(obj.keys())[0] in filters:\n",
    "            new_objects.append(obj)\n",
    "\n",
    "    # Keep only if there's a single object in the image\n",
    "    if len(new_objects) == 1:\n",
    "        annotation[\"class\"] = list(new_objects[0].keys())[0]\n",
    "        annotation[\"bbox\"] = list(new_objects[0].values())[0]\n",
    "        annotation.pop(\"objects\")\n",
    "        annotations.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images with annotations:\", len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Contents of annotation[0]:\\n\", annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Show the image corresponding to annotation[0]\n",
    "from skimage.io import imread\n",
    "from IPython.display import Image\n",
    "\n",
    "img = imread(\"VOCdevkit/VOC2007/JPEGImages/\" + annotations[0][\"filename\"])\n",
    "print(\"Image shape:\", img.shape)\n",
    "Image(\"VOCdevkit/VOC2007/JPEGImages/\" + annotations[0][\"filename\"])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display bounding box on top of the image\n",
    "from skimage.io import imshow\n",
    "from skimage.draw import rectangle_perimeter\n",
    "\n",
    "def draw_bbox(img, bbox):\n",
    "    img = img.copy()\n",
    "    rr, cc = rectangle_perimeter((bbox[1], bbox[0]), (bbox[3], bbox[2]), shape=img.shape)\n",
    "    img[rr, cc] = 0\n",
    "    return img\n",
    "\n",
    "imshow(draw_bbox(img, annotations[0][\"bbox\"]))\n",
    "print(f'Class: {annotations[0][\"class\"]}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correspondence between indices and labels:\\n\", idx2labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-computing representations\n",
    "\n",
    "Before designing the object detection model itself, we will use a pre-trained model to precompute the convolutional representations of the images. This will allow us to train the object detection model much faster. In simpler terms, what we are doing is using a pre-trained model to extract features from the images, and then we will train a simple model to predict the class and bounding box of the object in the image using these features.\n",
    "\n",
    "In the following cell, we'll download a pre-trained ResNet model and remove the last layers of the model to keep only the convolutional part. We will then use this model to precompute the representations of the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = ResNet50(include_top=False)\n",
    "input = model.layers[0].input\n",
    "\n",
    "# Remove the average pooling layer\n",
    "output = model.layers[-2].output\n",
    "headless_conv = Model(inputs=input, outputs=output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on a batch of images\n",
    "\n",
    "The `predict_batch` function is defined as follows:\n",
    "- open each image, and resize them to `img_size`\n",
    "- stack them as a batch tensor of shape `(batch, img_size_x, img_size_y, 3)`\n",
    "- preprocess the batch and make a forward pass with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "def predict_batch(model, img_batch_path, img_size=None):\n",
    "    img_list = []\n",
    "\n",
    "    for im_path in img_batch_path:\n",
    "        img = imread(im_path)\n",
    "        if img_size:\n",
    "            img = resize(img, img_size, mode='reflect', preserve_range=True)\n",
    "\n",
    "        img = img.astype('float32')\n",
    "        img_list.append(img)\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        raise ValueError(\n",
    "            'when both img_size and crop_size are None, all images '\n",
    "            'in image_paths must have the same shapes.')\n",
    "\n",
    "    return model(preprocess_input(img_batch)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model:"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Helper function to download images from our GitHub repo\n",
    "\n",
    "def download_image(image_name):\n",
    "    url = \"https://raw.githubusercontent.com/UofT-DSI/deep_learning/main/notebooks/images/\" + image_name\n",
    "    import requests\n",
    "    r = requests.get(url)\n",
    "    with open(image_name, 'wb') as f:\n",
    "        f.write(r.content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "download_image('bumblebee.png')\n",
    "Image('bumblebee.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict_batch(headless_conv, [\"bumblebee.png\"], (1000, 224))\n",
    "print(\"output shape\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output size is `(batch_size, 1000/32 = 32, 224/32 = 7, 2048)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute representations on all images in our annotations\n",
    "\n",
    "Computing representations for all images may take some time (especially without a GPU), so it was pre-computed and save in `voc_representations.h5`\n",
    "\n",
    "We will load the representations from the file `voc_representations.h5` and store them in a numpy array `reprs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('voc_representations.h5', 'r') as h5f:\n",
    "    reprs = h5f['reprs'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ground truth from annotation\n",
    "\n",
    "We cannot use directly the annotation dictionary as ground truth in our model. This is because the model will output a tensor of shape `(batch, num_classes)` for the classes and a tensor of shape `(batch, 4)` for the boxes coordinates. We will build the `y_true` tensor that will be compared to the output of the model.\n",
    "\n",
    "#### Boxes coordinates\n",
    "\n",
    "- The image is resized to a fixed 224x224 to be fed to the usual ResNet50 input, the boxes coordinates of the annotations need to be resized accordingly.\n",
    "- We have to convert the top-left and bottom-right coordinates `(x1, y1, x2, y2)` to center, height, width `(xc, yc, w, h)`\n",
    "\n",
    "#### Classes labels\n",
    "\n",
    "- The class labels are mapped to corresponding indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resize = 224\n",
    "num_classes = len(labels2idx.keys())\n",
    "\n",
    "\n",
    "def tensorize_ground_truth(annotations):\n",
    "    all_boxes = []\n",
    "    all_cls = []\n",
    "    for idx, annotation in enumerate(annotations):\n",
    "        # Build a one-hot encoding of the class\n",
    "        cls = np.zeros((num_classes))\n",
    "        cls_idx = labels2idx[annotation[\"class\"]]\n",
    "        cls[cls_idx] = 1.0\n",
    "        \n",
    "        coords = annotation[\"bbox\"]\n",
    "        size = annotation[\"size\"]\n",
    "        # resize the image\n",
    "        x1, y1, x2, y2 = (coords[0] * img_resize / size[0],\n",
    "                          coords[1] * img_resize / size[1], \n",
    "                          coords[2] * img_resize / size[0],\n",
    "                          coords[3] * img_resize / size[1])\n",
    "        \n",
    "        # compute center of the box and its height and width\n",
    "        cx, cy = ((x2 + x1) / 2, (y2 + y1) / 2)\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        boxes = np.array([cx, cy, w, h])\n",
    "        all_boxes.append(boxes)\n",
    "        all_cls.append(cls)\n",
    "\n",
    "    # stack everything into two big np tensors\n",
    "    return np.vstack(all_cls), np.vstack(all_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, boxes = tensorize_ground_truth(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes and boxes shapes:\", classes.shape, boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 2 classes labels:\\n\")\n",
    "print(classes[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 2 boxes coordinates:\\n\")\n",
    "print(boxes[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting output of model\n",
    "\n",
    "Interpreting the output of the model is going from the output tensors to a set of classes (with confidence) and boxes coordinates. It corresponds to reverting the previous process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_output(cls, boxes, img_size=(500, 333)):\n",
    "    cls_idx = np.argmax(cls)\n",
    "    confidence = cls[cls_idx]\n",
    "    classname = idx2labels[cls_idx]\n",
    "    cx, cy = boxes[0], boxes[1]\n",
    "    w, h = boxes[2], boxes[3]\n",
    "    \n",
    "    small_box = [max(0, cx - w / 2), max(0, cy - h / 2), \n",
    "                 min(img_resize, cx + w / 2), min(img_resize, cy + h / 2)]\n",
    "    \n",
    "    fullsize_box = [int(small_box[0] * img_size[0] / img_resize), \n",
    "                    int(small_box[1] * img_size[1] / img_resize),\n",
    "                    int(small_box[2] * img_size[0] / img_resize), \n",
    "                    int(small_box[3] * img_size[1] / img_resize)]\n",
    "    output = {\"class\": classname, \"confidence\": confidence, \"bbox\": fullsize_box}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check**: interpret the classes and boxes tensors of some known annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 1\n",
    "\n",
    "print(\"Original annotation:\\n\")\n",
    "print(annotations[img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interpreted output:\\n\")\n",
    "print(interpret_output(classes[img_idx], boxes[img_idx],\n",
    "                       img_size=annotations[img_idx][\"size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union\n",
    "\n",
    "In order to assess the quality of our model, we will monitor the IoU between ground truth box and predicted box. \n",
    "The following function computes the IoU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    # find the intersecting box coordinates\n",
    "    x0 = max(boxA[0], boxB[0])\n",
    "    y0 = max(boxA[1], boxB[1])\n",
    "    x1 = min(boxA[2], boxB[2])\n",
    "    y1 = min(boxA[3], boxB[3])\n",
    "    \n",
    "    # compute the area of intersection rectangle\n",
    "    inter_area = max(x1 - x0, 0) * max(y1 - y0, 0)\n",
    "\n",
    "    # compute the area of each box\n",
    "    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    " \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of areas - the interesection area\n",
    "    return inter_area / float(boxA_area + boxB_area - inter_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou([47, 35, 147, 101], [1, 124, 496, 235])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou([47, 35, 147, 101], [47, 35, 147, 101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou([47, 35, 147, 101], [49, 36, 145, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Localization model\n",
    "\n",
    "A two-headed model for single object classification and localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Input, Dropout, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def classif_and_loc_bad_model(num_classes):\n",
    "    \"\"\"bad model that averages all the spatial information\"\"\"\n",
    "    \n",
    "    model_input = Input(shape=(7, 7, 2048))\n",
    "    x = GlobalAveragePooling2D()(model_input) # We aren't doing any convolutional operation\n",
    "    \n",
    "    # Now we build two separate heads for the model: one for classification and one for localization\n",
    "    # Each takes in the output of the global average pooling layer\n",
    "    class_prediction_head = Dense(num_classes, activation=\"softmax\", name=\"head_classes\")(x)\n",
    "    \n",
    "    box_prediction_head = Dense(4, name=\"head_boxes\")(x)\n",
    "    \n",
    "    # Note that our model has two outputs\n",
    "    model = Model(model_input, outputs=[class_prediction_head, box_prediction_head],\n",
    "                  name=\"resnet_loc\")\n",
    "    model.compile(optimizer=\"adam\", loss=[categorical_crossentropy, \"mse\"],\n",
    "                  loss_weights=[1., 0.01]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_model = classif_and_loc_bad_model(num_classes)\n",
    "bad_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's debug the model: select only a few examples and test the model before training with random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 64\n",
    "inputs = reprs[0:num]\n",
    "out_cls, out_boxes = classes[0:num], boxes[0:num]\n",
    "\n",
    "print(\"Input batch shape:\", inputs.shape)\n",
    "print(\"Ground truth batch shapes:\", out_cls.shape, out_boxes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the classes are approximately balanced (except class 2 which is 'bus'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cls.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bad_model(inputs)\n",
    "print(\"model output shapes:\", out[0].shape, out[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check whether the loss decreases and eventually if we are able to overfit on these few examples for debugging purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bad_model.fit(inputs, [out_cls, out_boxes],\n",
    "                           batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log(history.history[\"head_boxes_loss\"]), label=\"boxes_loss\")\n",
    "plt.plot(np.log(history.history[\"head_classes_loss\"]), label=\"classes_loss\")\n",
    "plt.plot(np.log(history.history[\"loss\"]), label=\"loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Displaying images and bounding box\n",
    "\n",
    "In order to display our annotations, we build the function `plot_annotations` as follows:\n",
    "- display the image\n",
    "- display on top annotations and ground truth bounding boxes and classes\n",
    "\n",
    "The `display` function:\n",
    "- takes a single index and computes the result of the model\n",
    "- interpret the output of the model as a bounding box\n",
    "- calls the `plot_annotations` function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def patch(axis, bbox, display_txt, color):\n",
    "    coords = (bbox[0], bbox[1]), bbox[2]-bbox[0]+1, bbox[3]-bbox[1]+1\n",
    "    axis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "    axis.text(bbox[0], bbox[1], display_txt, color='white',\n",
    "              bbox={'facecolor': color, 'alpha': 0.7})\n",
    "\n",
    "\n",
    "def plot_annotations(img_path, annotation=None, ground_truth=None, figsize=(10, 8)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    img = imread(img_path)\n",
    "    ax.imshow(img)\n",
    "    if ground_truth:\n",
    "        text = \"gt \" + ground_truth[\"class\"]\n",
    "        patch(ax, ground_truth[\"bbox\"], text, \"red\")\n",
    "    if annotation:\n",
    "        conf = '{:0.2f} '.format(annotation['confidence'])\n",
    "        text = conf + annotation[\"class\"]\n",
    "        patch(ax, annotation[\"bbox\"], text, \"blue\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_prediction(model, index, ground_truth=True):\n",
    "    res = model(reprs[index][np.newaxis,])\n",
    "    output = interpret_output(res[0][0], res[1][0], img_size=annotations[index][\"size\"])\n",
    "    plot_annotations(\"VOCdevkit/VOC2007/JPEGImages/\" + annotations[index][\"filename\"], \n",
    "                     output, annotations[index] if ground_truth else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the predictions of the model and the ground truth annotation for a couple of images in our tiny debugging training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(bad_model, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class should be right but the localization has little chance to be correct.\n",
    "\n",
    "The model has even more trouble on images that were not part of our tiny debugging training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(bad_model, 189)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Accuracy\n",
    "\n",
    "For each example `(class_true, bbox_true)`, we consider it positive if and only if:\n",
    "- the argmax of `output_class` of the model is `class_true`\n",
    "- the IoU between the `output_bbox` and the `bbox_true` is above a threshold (usually `0.5`)\n",
    "\n",
    "The accuracy of a model is then number of positive / total_number\n",
    "\n",
    "The following functions compute the class accuracy, iou average and global accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class accuracy, iou average and global accuracy\n",
    "def accuracy_and_iou(preds, trues, threshold=0.5):\n",
    "    sum_valid, sum_accurate, sum_iou = 0, 0, 0\n",
    "    num = len(preds)\n",
    "    for pred, true in zip(preds, trues):\n",
    "        iou_value = iou(pred[\"bbox\"], true[\"bbox\"])\n",
    "        if pred[\"class\"] == true[\"class\"] and iou_value > threshold:\n",
    "            sum_valid = sum_valid + 1\n",
    "        sum_iou = sum_iou + iou_value\n",
    "        if pred[\"class\"] == true[\"class\"]:\n",
    "            sum_accurate = sum_accurate + 1\n",
    "    return sum_accurate / num, sum_iou / num, sum_valid / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the previous function on the whole train / test set\n",
    "def compute_acc(model, train=True):\n",
    "    n_samples = len(annotations)\n",
    "    if train:\n",
    "        beg, end = 0, (9 * n_samples // 10)\n",
    "        split_name = \"train\"\n",
    "    else:\n",
    "        beg, end = (9 * n_samples) // 10, n_samples \n",
    "        split_name = \"test\"\n",
    "    res = model.predict(reprs[beg:end])\n",
    "    outputs = []\n",
    "    for index, (classes, boxes) in enumerate(zip(res[0], res[1])):\n",
    "        output = interpret_output(classes, boxes,\n",
    "                                  img_size=annotations[index][\"size\"])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    acc, iou, valid = accuracy_and_iou(outputs, annotations[beg:end],\n",
    "                                       threshold=0.5)\n",
    "    \n",
    "    print('[{}] class accuracy: {:0.3f}, mean IoU: {:0.3f},'\n",
    "          ' valid accuracy: {:0.3f}'.format(\n",
    "            split_name, acc, iou, valid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_acc(bad_model, train=True)\n",
    "compute_acc(bad_model, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class accuracy is not too bad. What is the chance level for this problem? The localization measure by IoU is really bad.\n",
    "\n",
    "\n",
    "### Training on the whole dataset\n",
    "\n",
    "We split our dataset into a train and a test dataset.\n",
    "\n",
    "Then train the model on the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep last examples for test\n",
    "test_num = reprs.shape[0] // 10\n",
    "train_num = reprs.shape[0] - test_num\n",
    "test_inputs = reprs[train_num:]\n",
    "test_cls, test_boxes = classes[train_num:], boxes[train_num:]\n",
    "print(train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_model = classif_and_loc_bad_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "inputs = reprs[0:train_num]\n",
    "out_cls, out_boxes = classes[0:train_num], boxes[0:train_num]\n",
    "\n",
    "history = bad_model.fit(inputs, y=[out_cls, out_boxes], \n",
    "                           validation_data=(test_inputs, [test_cls, test_boxes]), \n",
    "                           batch_size=batch_size, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_acc(bad_model, train=True)\n",
    "compute_acc(bad_model, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class accuracy is quite good. The **localization quality measured by IoU** is slightly better that previously but still **poor even when measured on the training set**. The poor design of the localization head is causing the model to under fit.\n",
    "\n",
    "\n",
    "### Build a better model\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Use any tool at your disposal to build a better model:\n",
    "- `Dropout`\n",
    "- `Convolution2D`, `Dense`, with activations functions\n",
    "- `Flatten`, `GlobalAveragePooling2D`, `GlobalMaxPooling2D`, etc.\n",
    "\n",
    "Notes:\n",
    "- Be careful not to add too parametrized layers as you only have ~1200 training samples\n",
    "- Feel free to modify hyperparameters: learning rate, optimizers, loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def classif_and_loc(num_classes):\n",
    "    model_input = Input(shape=(7, 7, 2048))\n",
    "    \n",
    "    # TODO: Build a better model. Remember that you have two heads: one for classification and one for localization\n",
    "    \n",
    "    # add some stuff that works directly on `model_input` here\n",
    "    \n",
    "    # then build the two separate heads\n",
    "    \n",
    "    # class_prediction_head = ...\n",
    "        \n",
    "    # box_prediction_head = ...\n",
    "        \n",
    "    model = Model(model_input, outputs=[class_prediction_head, box_prediction_head], name=\"resnet_loc\")\n",
    "    model.compile(optimizer=\"adam\", loss=[categorical_crossentropy, \"mse\"],\n",
    "                  loss_weights=[1., 1 / (224 * 224)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Build, train and compute accuracy\n",
    "better_model = classif_and_loc(5)\n",
    "\n",
    "history = better_model.fit(x = inputs, y=[out_cls, out_boxes],\n",
    "                           validation_data=(test_inputs, [test_cls, test_boxes]),\n",
    "                           batch_size=batch_size, epochs=30, verbose=2)\n",
    "\n",
    "compute_acc(better_model, train=True)\n",
    "compute_acc(better_model, train=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(better_model, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(bad_model, 11)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "display_prediction(better_model, 52)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "display_prediction(bad_model, 52)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
