{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation of Numeric Phrases with Seq2Seq\n",
    "\n",
    "In the following we will try to build a **translation model from english phrases describing numbers** to the corresponding **numeric representation** (base 10).\n",
    "\n",
    "This is a toy machine translation task with a **restricted vocabulary** and a **single valid translation for each source phrase** which makes it more tractable to train on a laptop computer and easier to evaluate. Despite those limitations we expect that this task will highlight interesting properties of Seq2Seq models including:\n",
    "\n",
    "- the ability to **deal with different length** of the source and target sequences,\n",
    "- handling token with a **meaning that changes depending on the context** (e.g \"one\" as a number or as a pronoun),\n",
    "- basic counting and \"reasoning\" capabilities of LSTM and GRU models.\n",
    "\n",
    "The parallel text data is generated from a \"ground-truth\" Python function named `to_english_phrase` that captures common rules. Hyphenation was intentionally omitted to make the phrases more ambiguous and therefore make the translation problem slightly harder to solve (and also because Olivier had no particular interest hyphenation in properly implementing rules :)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from english_numbers import to_english_phrase\n",
    "    \n",
    "for x in [21, 80, 81, 300, 213, 1100, 1201, 301000, 80080]:\n",
    "    print(str(x).rjust(6), to_english_phrase(x))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Training Set\n",
    "\n",
    "The following will **generate phrases 20000 example phrases for numbers between 1 and 1,000,000** (excluded). We chose to over-represent small numbers by generating all the possible short sequences between `1` and `exhaustive=5000`.\n",
    "\n",
    "We then split the generated set into non-overlapping train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from english_numbers import generate_translations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "numbers, english_numbers = generate_translations(\n",
    "    low=1, high=int(1e6) - 1, exhaustive=5000, random_seed=0)\n",
    "num_train, num_dev, en_train, en_dev = train_test_split(\n",
    "    numbers, english_numbers, test_size=0.5, random_state=0)\n",
    "\n",
    "num_val, num_test, en_val, en_test = train_test_split(\n",
    "    num_dev, en_dev, test_size=0.5, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(en_train), len(en_val), len(en_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i, en_phrase, num_phrase in zip(range(5), en_train, num_train):\n",
    "    print(num_phrase.rjust(6), en_phrase)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i, en_phrase, num_phrase in zip(range(5), en_val, num_val):\n",
    "    print(num_phrase.rjust(6), en_phrase)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabularies\n",
    "\n",
    "Build the vocabularies from the training set only to get a chance to have some out-of-vocabulary words in the validation and test sets.\n",
    "\n",
    "First we need to introduce specific symbols that will be used to:\n",
    "- pad sequences\n",
    "- mark the beginning of translation\n",
    "- mark the end of translation\n",
    "- be used as a placehold for out-of-vocabulary symbols (not seen in the training set).\n",
    "\n",
    "Here we use the same convention as the [tensorflow seq2seq tutorial](https://www.tensorflow.org/tutorials/seq2seq):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "PAD, GO, EOS, UNK = START_VOCAB = ['_PAD', '_GO', '_EOS', '_UNK']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the vocabulary we need to tokenize the sequences of symbols. For the digital number representation we use character level tokenization while whitespace-based word level tokenization will do for the english phrases:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def tokenize(sentence, word_level=True):\n",
    "    if word_level:\n",
    "        return sentence.split()\n",
    "    else:\n",
    "        return [sentence[i:i + 1] for i in range(len(sentence))]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenize('1234', word_level=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenize('one thousand two hundred thirty four', word_level=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this tokenization strategy to assign a unique integer token id to each possible token string found the traing set in each language ('english' and 'numeric'): "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_vocabulary(tokenized_sequences):\n",
    "    rev_vocabulary = START_VOCAB[:]\n",
    "    unique_tokens = set()\n",
    "    for tokens in tokenized_sequences:\n",
    "        unique_tokens.update(tokens)\n",
    "    rev_vocabulary += sorted(unique_tokens)\n",
    "    vocabulary = {}\n",
    "    for i, token in enumerate(rev_vocabulary):\n",
    "        vocabulary[token] = i\n",
    "    return vocabulary, rev_vocabulary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenized_en_train = [tokenize(s, word_level=True) for s in en_train]\n",
    "tokenized_num_train = [tokenize(s, word_level=False) for s in num_train]\n",
    "\n",
    "en_vocab, rev_en_vocab = build_vocabulary(tokenized_en_train)\n",
    "num_vocab, rev_num_vocab = build_vocabulary(tokenized_num_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two languages do not have the same vocabulary sizes:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(en_vocab)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(num_vocab)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for k, v in sorted(en_vocab.items())[:10]:\n",
    "    print(f'{k:10}: {v}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for k, v in sorted(num_vocab.items()):\n",
    "    print(f'{k:10}: {v}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also built the reverse mappings from token ids to token string representations:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(rev_en_vocab)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(rev_num_vocab)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq with a single GRU architecture\n",
    "\n",
    "<img src=\"images/basic_seq2seq.png\" width=\"80%\" />\n",
    "\n",
    "From: [Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" NIPS 2014](https://arxiv.org/abs/1409.3215)\n",
    "\n",
    "\n",
    "\n",
    "For a given source sequence - target sequence pair, we will:\n",
    "- tokenize the source and target sequences;\n",
    "- reverse the order of the source sequence;\n",
    "- build the input sequence by concatenating the reversed source sequence and the target sequence in original order using the `_GO` token as a delimiter, \n",
    "- build the output sequence by appending the `_EOS` token to the source sequence.\n",
    "\n",
    "\n",
    "Let's do this as a function using the original string representations for the tokens so as to make it easier to debug:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- Write a function that turns a pair of tokenized (source, target) sequences into a pair of (input, output) sequences as described above.\n",
    "- The function should have a `reverse_source=True` as an option.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "input_tokens, output_tokens = make_input_output(\n",
    "    ['one', 'hundred', 'twenty', 'two'],\n",
    "    ['1', '2', '2'],\n",
    "    reverse_source=True\n",
    ")\n",
    "==> input_tokens == ['two', 'twenty', 'hundred', 'one', '_GO', '1', '2', '2']\n",
    "==> output_tokens == ['1', '2', '2', '_EOS']\n",
    "\n",
    "input_tokens, output_tokens = make_input_output(\n",
    "    ['one', 'hundred', 'twenty', 'two'],\n",
    "    ['1', '2', '2'],\n",
    "    reverse_source=False)\n",
    "==> input_tokens == ['one', 'hundred', 'twenty', 'two', '_GO', '1', '2', '2']\n",
    "==> output_tokens == ['1', '2', '2', '_EOS']\n",
    "```\n",
    "\n",
    "\n",
    "Notes: \n",
    "- The function should output two sequences of string tokens: one to be fed as the input and the other as expected output for the seq2seq network.\n",
    "- Do not pad the sequences: we will handle the padding later.\n",
    "- Don't forget to insert the `_GO` and `_EOS` special symbols at the right locations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def make_input_output(source_tokens, target_tokens, reverse_source=True):\n",
    "    # Your code here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input_tokens, output_tokens = make_input_output(\n",
    "    ['one', 'hundred', 'twenty', 'one'],\n",
    "    ['1', '2', '1'],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input_tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "output_tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of the parallel corpus\n",
    "\n",
    "Let's apply the previous transformation to each pair of (source, target) sequene and use a shared vocabulary to store the results in numpy arrays of integer token ids, with padding on the left so that all input / output sequences have the same length: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_tokenized_sequences = tokenized_en_train + tokenized_num_train\n",
    "shared_vocab, rev_shared_vocab = build_vocabulary(all_tokenized_sequences)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "max(len(s) for s in tokenized_en_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "max(len(s) for s in tokenized_num_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "max_length = 20  # found by introspection of our training set\n",
    "\n",
    "def vectorize_corpus(source_sequences, target_sequences, shared_vocab,\n",
    "                     word_level_source=True, word_level_target=True,\n",
    "                     max_length=max_length):\n",
    "    assert len(source_sequences) == len(target_sequences)\n",
    "    n_sequences = len(source_sequences)\n",
    "    source_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    source_ids.fill(shared_vocab[PAD])\n",
    "    target_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    target_ids.fill(shared_vocab[PAD])\n",
    "    numbered_pairs = zip(range(n_sequences), source_sequences, target_sequences)\n",
    "    for i, source_seq, target_seq in numbered_pairs:\n",
    "        source_tokens = tokenize(source_seq, word_level=word_level_source)\n",
    "        target_tokens = tokenize(target_seq, word_level=word_level_target)\n",
    "        \n",
    "        in_tokens, out_tokens = make_input_output(source_tokens, target_tokens)\n",
    "        \n",
    "        in_token_ids = [shared_vocab.get(t, UNK) for t in in_tokens]\n",
    "        source_ids[i, -len(in_token_ids):] = in_token_ids\n",
    "    \n",
    "        out_token_ids = [shared_vocab.get(t, UNK) for t in out_tokens]\n",
    "        target_ids[i, -len(out_token_ids):] = out_token_ids\n",
    "    return source_ids, target_ids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train, Y_train = vectorize_corpus(en_train, num_train, shared_vocab,\n",
    "                                    word_level_target=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Y_train.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "en_train[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_train[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Y_train[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. In particular we can note:\n",
    "\n",
    "- the PAD=0 symbol at the beginning of the two sequences,\n",
    "- the input sequence has the GO=1 symbol to separate the source from the target,\n",
    "- the output sequence is a shifted version of the target and ends with EOS=2.\n",
    "\n",
    "Let's vectorize the validation and test set to be able to evaluate our models:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_val, Y_val = vectorize_corpus(en_val, num_val, shared_vocab,\n",
    "                                word_level_target=False)\n",
    "X_test, Y_test = vectorize_corpus(en_test, num_test, shared_vocab,\n",
    "                                  word_level_target=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_val.shape, Y_val.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_test.shape, Y_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple homogeneous Seq2Seq architecture\n",
    "\n",
    "To keep the architecture simple we will use the **same RNN model and weights for both the encoder part** (before the `_GO` token) **and the decoder part** (after the `_GO` token).\n",
    "\n",
    "We may GRU recurrent cell instead of LSTM because it is slightly faster to compute and should give comparable results.\n",
    "\n",
    "**Exercise:**\n",
    "- Build a Seq2Seq model:\n",
    "  - Start with an Embedding layer;\n",
    "  - Add a single GRU layer: the GRU layer should yield a sequence of output vectors, one at each timestep;\n",
    "  - Add a Dense layer to adapt the ouput dimension of the GRU layer to the dimension of the output vocabulary;\n",
    "  - Don't forget to insert some Dropout layer(s), especially after the Embedding layer.\n",
    "\n",
    "Note:\n",
    "- The output dimension of the Embedding layer should be smaller than usual be cause we have small vocabulary size;\n",
    "- The dimension of the GRU should be larger to give the Seq2Seq model enough \"working memory\" to memorize the full input sequence before decoding it;\n",
    "- Your model should output a shape `[batch, sequence_length, vocab_size]`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, GRU, Dense\n",
    "\n",
    "VOCAB_SIZE = len(shared_vocab)\n",
    "\n",
    "simple_seq2seq = Sequential([\n",
    "    # Your Code Here\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's register a callback mechanism to automatically snapshot the best model by measure the performance of the model on the validation set at the end of each epoch during training:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "best_model_fname = \"simple_seq2seq_checkpoint\"\n",
    "best_model_cb = ModelCheckpoint(best_model_fname, monitor='val_loss',\n",
    "                                save_best_only=True, verbose=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We need to use np.expand_dims trick on Y: this is required by Keras because we use a sparse (integer-based) representation for the output:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = simple_seq2seq.fit(X_train, np.expand_dims(Y_train, -1),\n",
    "                             validation_data=(X_val, np.expand_dims(Y_val, -1)),\n",
    "                             epochs=15, verbose=2, batch_size=32,\n",
    "                             callbacks=[best_model_cb])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], '--', label='validation')\n",
    "plt.ylabel('negative log likelihood')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('Convergence plot for Simple Seq2Seq')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the best model found on the validation set at the end of training:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "simple_seq2seq = load_model(best_model_fname)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a raw prediction on the first sample of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "en_test[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numeric array this is provided (along with the expected target sequence) as the following padded input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "first_test_sequence = X_test[0:1]\n",
    "first_test_sequence"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the `_GO` (symbol indexed at `1`) separates the reversed source from the expected target sequence:  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rev_shared_vocab[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "To use the model to make new predictions, we need to write a method that successively predicts one token at a time until the end of sequence symbol is reached. This is provided for you below:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def greedy_translate(model, source_sequence, shared_vocab, rev_shared_vocab,\n",
    "                     word_level_source=True, word_level_target=True):\n",
    "    \"\"\"Greedy decoder recursively predicting one token at a time\"\"\"\n",
    "    \n",
    "    # Initialize the list of input token ids with the source sequence\n",
    "    source_tokens = tokenize(source_sequence, word_level=word_level_source)\n",
    "    input_ids = [shared_vocab.get(t, UNK) for t in reversed(source_tokens)]\n",
    "    input_ids += [shared_vocab[GO]]\n",
    "\n",
    "    # Prepare a fixed size numpy array that matches the expected input\n",
    "    # shape for the model\n",
    "    input_array = np.empty(shape=(1, model.input_shape[1]),\n",
    "                           dtype=np.int32)\n",
    "    decoded_tokens = []\n",
    "    while len(input_ids) <= max_length:\n",
    "        # Vectorize the list of input tokens and use zero padding.\n",
    "        input_array.fill(shared_vocab[PAD])\n",
    "        input_array[0, -len(input_ids):] = input_ids\n",
    "        \n",
    "        # Predict the next output: greedy decoding with argmax\n",
    "        next_token_id = model(input_array)[0, -1].numpy().argmax()\n",
    "        \n",
    "        # Stop decoding if the network predicts end of sentence:\n",
    "        if next_token_id == shared_vocab[EOS]:\n",
    "            break\n",
    "            \n",
    "        # Otherwise use the reverse vocabulary to map the prediction\n",
    "        # back to the string space\n",
    "        decoded_tokens.append(rev_shared_vocab[next_token_id])\n",
    "        \n",
    "        # Append prediction to input sequence to predict the next\n",
    "        input_ids.append(next_token_id)\n",
    "\n",
    "    separator = \" \" if word_level_target else \"\"\n",
    "    return separator.join(decoded_tokens)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "phrases = [\n",
    "    \"one\",\n",
    "    \"two\",\n",
    "    \"three\",\n",
    "    \"eleven\",\n",
    "    \"fifteen\",\n",
    "    \"one hundred thirty two\",\n",
    "    \"one hundred thousand twelve\",\n",
    "    \"seven thousand eight hundred fifty nine\",\n",
    "    \"twenty one\",\n",
    "    \"twenty four\",\n",
    "    \"eighty\",\n",
    "    \"ninety one thousand\",\n",
    "    \"ninety one thousand two hundred two\",\n",
    "]\n",
    "for phrase in phrases:\n",
    "    translation = greedy_translate(simple_seq2seq, phrase,\n",
    "                                   shared_vocab, rev_shared_vocab,\n",
    "                                   word_level_target=False)\n",
    "    print(phrase.ljust(40), translation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The results are far from perfect but we can see that the network has already picked up some translation skills. Let's see what happens when we try to translate some phrases that were not in the training set:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "phrases = [\n",
    "    \"one hundred twenty three\",\n",
    "    \"eleven thousand two hundred thirty four\",\n",
    "    \"fourteen hundred\"\n",
    "]\n",
    "for phrase in phrases:\n",
    "    translation = greedy_translate(simple_seq2seq, phrase,\n",
    "                                   shared_vocab, rev_shared_vocab,\n",
    "                                   word_level_target=False)\n",
    "    print(phrase.ljust(40), translation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Because **we expect only one correct translation** for a given source sequence, we can use **phrase-level accuracy** as a metric to quantify our model quality.\n",
    "\n",
    "Note that **this is not the case for real translation models** (e.g. from English to French on arbitrary sentences). Evaluation of a machine translation model is tricky in general. Automated evaluation can somehow be done at the corpus level with the [BLEU score](https://en.wikipedia.org/wiki/BLEU) (bilingual evaluation understudy) given a large enough sample of correct translations provided by certified translators but its only a noisy proxy.\n",
    "\n",
    "The only good evaluation is to give a large enough sample of the model predictions on some test sentences to certified translators and ask them to give an evaluation (e.g. a score between 0 and 6, 0 for non-sensical and 6 for the hypothetical perfect translation). However in practice this is very costly to do.\n",
    "\n",
    "Fortunately we can just use phrase-level accuracy on a our very domain specific toy problem:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def phrase_accuracy(model, num_sequences, en_sequences, n_samples=300,\n",
    "                    decoder_func=greedy_translate):\n",
    "    correct = []\n",
    "    n_samples = len(num_sequences) if n_samples is None else n_samples\n",
    "    for i, num_seq, en_seq in zip(range(n_samples), num_sequences, en_sequences):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Decoding %d/%d\" % (i, n_samples))\n",
    "\n",
    "        predicted_seq = decoder_func(simple_seq2seq, en_seq,\n",
    "                                     shared_vocab, rev_shared_vocab,\n",
    "                                     word_level_target=False)\n",
    "        correct.append(num_seq == predicted_seq)\n",
    "    return np.mean(correct)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Phrase-level train accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_train, en_train))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Phrase-level test accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_test, en_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "We've only scratched the surface of what can be done with Seq2Seq models. If you have time, see if you can improve the quality of the model further, to get a better accuracy on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
